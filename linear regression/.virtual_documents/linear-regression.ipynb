


import matplotlib.pyplot as plt
import numpy as np


plt.style.use('seaborn')





x = np.arange(-10, 10)
x


K = np.linspace(-3, 3, 6)
K


# Set up the subplots
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
fig.suptitle('Variation Examples: Direct to Inverse', fontsize=24)

for i, k in enumerate(K):
    row = 0 if i<3 else 1
    col = i%3
    y = k*x
    name_type = "Direct" if k>0 else "Inverse"
    # Plot 1: Direct Variation
    axes[row, col].plot(x, y, color='blue', linestyle='dashed')
    axes[row, col].scatter(x, y, color='blue')
    axes[row, col].set_title(f'{name_type}: $y={k:.2f} x$', fontsize=16)
    axes[row, col].set_xlabel('x')
    axes[row, col].set_ylabel('y')
    axes[row, col].set_xlim(-10, 10)
    axes[row, col].set_ylim(-10, 10)
plt.savefig('variation.png')





# Set up the subplots
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
fig.suptitle('Variation Examples: Direct to Inverse', fontsize=24)

b = 5
for i, k in enumerate(K):
    row = 0 if i<3 else 1
    col = i%3
    y = k*x + b
    name_type = "Direct" if k>0 else "Inverse"
    # Plot 1: Direct Variation
    axes[row, col].plot(x, y, color='blue', linestyle='dashed')
    axes[row, col].plot(0, 5, 'ks', markersize=10)
    axes[row, col].scatter(x, y, color='blue')
    axes[row, col].set_title(f'{name_type}: $y={k:.2f} x$', fontsize=16)
    axes[row, col].set_xlabel('x')
    axes[row, col].set_ylabel('y')
    axes[row, col].set_xlim(-10, 10)
    axes[row, col].set_ylim(-10, 10)
plt.savefig('variation2.png')











from sklearn.datasets import make_regression


## step 0: generate synthetic data for this experiment.
# Generate a synthetic linear dataset
X, y = make_regression(n_samples=100, n_features=10, noise=10, random_state=42)


X.shape, y.shape


## step 1-2: create randomly beta and show the X*beta
beta = np.random.rand(10)
X[0, :], beta


X[0, :] * beta


## step 3:
y_pred = X @ beta
y_pred


optimized_beta = np.linalg.inv(X.T @ X) @ X.T @ y


y_pred = X @ optimized_beta
y_pred





r_square = 1 - np.sum((y-y_pred)**2) / np.sum((y-np.mean(y))**2)
r_square



