import numpy as np
from sklearn import datasets
from sklearn import preprocessing
from sklearn import model_selection


features, targets = datasets.load_breast_cancer(return_X_y=True)


features.shape


x_train, x_test, y_train, y_test = model_selection.train_test_split(features, targets, test_size=0.2, random_state=0)


n_features = x_train.shape[-1]
n_features


# Input features are centered and scaled to unit variance:
feature_encoder = preprocessing.StandardScaler()
x_train = feature_encoder.fit_transform(x_train)
x_test = feature_encoder.transform(x_test)


x_train.shape[0], x_test.shape[0]


# number of neurons per layer, the last element must be 1
n_neurons = np.array([100, 10, 1])
n_branches = 20  # number of dendritic brancher per neuron


n_neurons





import torch.nn as nn
import torch


class DentricLayer(nn.Module):
    def __init__(self, 
                 n_input:int,
                 n_output:int, 
                 n_branches:int,
                 hyperplane_bias_magnitude:float = 1.):
        
        super(DentricLayer, self).__init__()
        
        self.dgn_weights = nn.Parameter(
            torch.zeros(n_output, n_branches, n_input + 1),
            requires_grad=False,
        )
        self.dgn_hyperplanes = torch.normal(
            0, 1, [n_output, n_branches, n_input + 1],
            requires_grad=False,
        )
        norm = torch.linalg.norm(self.dgn_hyperplanes[:, :, 1:], axis=(1, 2))[:, None, None]
        self.dgn_hyperplanes = self.dgn_hyperplanes / norm
        
        self.hyperplane_bias_magnitude = torch.FloatTensor([hyperplane_bias_magnitude])
        
    def forward(self, X):
        x_in = torch.hstack([X, torch.ones((X.shape[0], 1))])
        filtered_x = torch.hstack([X, torch.ones((X.shape[0], 1)) * self.hyperplane_bias_magnitude])
        
        gate_values = torch.matmul(self.dgn_hyperplanes, filtered_x.T)
        gate_values = torch.heaviside(gate_values, torch.tensor(0.))
        
        effective_weights = torch.tensordot(gate_values, self.dgn_weights, dims=([1], [1])).sum(dim=0)
        r_out = torch.einsum('ijk,ki->ij', effective_weights, x_in.T)
        
        return r_out


class DGNModel(nn.Module):
    def __init__(self):
        super(DGNModel, self).__init__()
        
        self.l1 = DentricLayer(n_input=31,
                  n_branches=20, 
                  n_output=100)
        
        self.l2 = DentricLayer(n_input=100,
                  n_branches=20, 
                  n_output=10)
        
        self.l3 = DentricLayer(n_input=10,
                  n_branches=20, 
                  n_output=1)
        
    def forward(self, x):
        x = self.l1(x)
        x = self.l2(x)
        x = self.l3(x)
        return x


# model = nn.Sequential(
#     DentricLayer(n_input=31,
#     n_branches=20, 
#     n_output=100),
    
#     DentricLayer(n_input=100,
#     n_branches=20, 
#     n_output=10),
    
#     DentricLayer(n_input=10,
#     n_branches=20, 
#     n_output=1)
# )


model = DGNModel()


for l in model.parameters():
    print(l)


a = list(model.parameters())


y_hat = model(x)


y_hat.shape





def loss(r, target):
    return r - target

class optimizer(torch.optim.Optimizer):
    def __init__(self, params, lr=0.001):
        defaults = dict(lr=lr)
        super(optimizer, self).__init__(params, defaults)
        self.lr = {}

        for group in self.param_groups:
            for param in group['params']:
                self.lr[param] = torch.ones_like(param.data) * lr
    
    def step(self):
        pass


model.parameters()


for param in model.parameters():
    print(param)


L1 = DentricLayer(n_input=31,
                  n_branches=20, 
                  n_output=100)


for name, param in model.named_parameters():
    print(f"Name: {name}")


model._modules.get('l1')(x)


x = np.hstack([np.ones((x_train.shape[0], 1)), x_train])
x = torch.FloatTensor(x)
L1(x)



